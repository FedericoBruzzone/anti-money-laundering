{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    GITHUB_TOKEN = \"\"\n",
    "    !rm -rf anti_money_laundering\n",
    "    !git clone https://{GITHUB_TOKEN}@github.com/FedericoBruzzone/anti_money_laundering.git\n",
    "    !mv anti_money_laundering/* \n",
    "    !rm -rf anti_money_laundering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from src.utils.kaggle_config            import setup_kaggle\n",
    "from src.utils.kaggle_config            import download_dataset\n",
    "\n",
    "from src.utils.datasets_handler         import (get_train_and_test,\n",
    "                                                get_X_and_Y,\n",
    "                                                print_dataset,\n",
    "                                                label_encoder,\n",
    "                                                split_timestamp)\n",
    "from src.utils.performance_measures     import calculate_performances\n",
    "from src.utils.dataset_sampling_methods import (oversampling,\n",
    "                                                undersampling,\n",
    "                                                bootstrap_sampling)\n",
    "\n",
    "from src.utils.print_utils              import (printLBlue, printGreen)\n",
    "\n",
    "from src.decision_tree.decision_tree    import CustomDecisionTree\n",
    "from src.decision_tree.ID3              import DecisionTreeID3\n",
    "from src.decision_tree.C45              import DecisionTreeC45\n",
    "from src.decision_tree.entropy_type     import EntropyType\n",
    "from src.decision_tree.criterion_type   import CriterionType\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "VERBOSE = int(os.getenv('VERBOSE'))\n",
    "VIEW = os.getenv('VIEW')\n",
    "\n",
    "setup_kaggle()\n",
    "print(\"Downloading dataset...\") \n",
    "download_dataset(\"iammustafatz/diabetes-prediction-dataset\")\n",
    "download_dataset(\"ealtman2019/ibm-transactions-for-anti-money-laundering-aml\")\n",
    "print(\"Done.\")\n",
    "\n",
    "hi_small_trans = \"HI-Small_Trans.csv\"\n",
    "diabetes = \"diabetes_prediction_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary test: Diabetes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = get_train_and_test(diabetes, verbose=VERBOSE)\n",
    "X_train, y_train = get_X_and_Y(df_train, verbose=VERBOSE)\n",
    "X_test, y_test = get_X_and_Y(df_test, verbose=VERBOSE)\n",
    "X_train, _ = label_encoder(X_train, ['gender', 'smoking_history'])\n",
    "X_test,  _ = label_encoder(X_test, ['gender', 'smoking_history'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------------------- ID3 --------------------------\")    \n",
    "start_time = time.time()\n",
    "decision_tree: DecisionTreeID3 = DecisionTreeID3(max_depth=10, \n",
    "                                                    num_thresholds_numerical_attr=6)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "decision_tree.create_dot_files(filename=\"tree-id3-diabetes\", generate_png=True, view=\"\")\n",
    "print()\n",
    "print(\"Performances: \")\n",
    "predictions = list(decision_tree.predict_test(X_test))\n",
    "print(f\"Fit time: {end_time - start_time} seconds\") \n",
    "calculate_performances(predictions, y_test, \"id3\", verbose=True)\n",
    "print(\"-------------------------- END ID3 --------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='./dot_figs/tree-id3-diabetes.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-------------------------- CUSTOM --------------------------\")\n",
    "start_time = time.time()\n",
    "decision_tree = CustomDecisionTree(criterion=EntropyType.SHANNON, \n",
    "                                    type_criterion=CriterionType.BEST, \n",
    "                                    max_depth=10, \n",
    "                                    min_samples_split=20,\n",
    "                                    num_thresholds_numerical_attr=6)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "decision_tree.create_dot_files(filename=\"tree-custom-diabetes\", generate_png=True, view=\"\")\n",
    "print()\n",
    "print(\"Performances: \") \n",
    "predictions = list(decision_tree.predict_test(X_test))\n",
    "print(f\"Fit time: {end_time - start_time} seconds\")\n",
    "calculate_performances(predictions, y_test, \"custom\", verbose=True)\n",
    "print(\"-------------------------- END CUSTOM --------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='./dot_figs/tree-custom-diabetes.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBM Money Laundering Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df_train, original_df_test = get_train_and_test(hi_small_trans, verbose=VERBOSE)\n",
    "\n",
    "print(original_df_train[\"Is Laundering\"].value_counts())\n",
    "print(original_df_test[\"Is Laundering\"].value_counts())\n",
    "\n",
    "print(\"Length of training set:\", len(original_df_train), \"    Length of test set:\", len(original_df_test))\n",
    "\n",
    "original_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_timestamp(original_df_train)\n",
    "split_timestamp(original_df_test)\n",
    "\n",
    "original_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df_train, _ = label_encoder(original_df_train, ['Date', 'Account', 'Account.1', 'Receiving Currency', 'Payment Currency', 'Payment Format'])\n",
    "original_df_test, _ = label_encoder(original_df_test, ['Date', 'Account', 'Account.1', 'Receiving Currency', 'Payment Currency', 'Payment Format'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_n_thresholds_values = [2, 4, 6]\n",
    "hp_max_depth_values = [4, 8, 12]\n",
    "\n",
    "def id3_experiment(df_train, df_test, type):\n",
    "    X_train, y_train = get_X_and_Y(df_train, verbose=VERBOSE)\n",
    "    X_test, y_test = get_X_and_Y(df_test, verbose=VERBOSE)\n",
    "    \n",
    "    for hp_n_thresholds in hp_n_thresholds_values:\n",
    "        printGreen(f\"Number of thresholds for num attr: {hp_n_thresholds}\")\n",
    "\n",
    "        for hp_max_depth in hp_max_depth_values:\n",
    "            printGreen(f\"   Max depth: {hp_max_depth}\")\n",
    "            start_time = time.time()\n",
    "            decision_tree: DecisionTreeID3 = DecisionTreeID3(max_depth=hp_max_depth, num_thresholds_numerical_attr=hp_n_thresholds)\n",
    "            decision_tree.fit(X_train, y_train)\n",
    "            end_time = time.time()\n",
    "            decision_tree.create_dot_files(filename=f\"tree-id3-{type}-{hp_n_thresholds}\", generate_png=True, view=VIEW)\n",
    "            print(\"PERFORMANCES:\")\n",
    "            predictions = list(decision_tree.predict_test(X_test))\n",
    "        \n",
    "            calculate_performances(predictions, y_test, \"id3\", verbose=True)\n",
    "\n",
    "            print(\"\\nFit time: %.2f minutes\" % ((end_time - start_time) / 60))\n",
    "            print(\"Predict time: %.2f minutes\" % ((time.time() - end_time) / 60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printLBlue(\"Preprocessing: Undersampling\")\n",
    "df_train, df_test = original_df_train, original_df_test\n",
    "df_train = undersampling(df_train, VERBOSE=False)\n",
    "id3_experiment(df_train, df_test, \"undersampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printLBlue(\"Preprocessing: Oversampling\")\n",
    "df_train, df_test = original_df_train, original_df_test\n",
    "df_train = oversampling(df_train, VERBOSE=False)\n",
    "id3_experiment(df_train, df_test, \"oversampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printLBlue(\"Without preprocessing\")\n",
    "df_train, df_test = original_df_train, original_df_test\n",
    "id3_experiment(df_train, df_test, \"wo_preprocessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_max_depth_values = [4, 7, 10]\n",
    "hp_n_thresholds_values = [2, 4, 6]\n",
    "hp_min_samples_split_values = [2, 100]\n",
    "\n",
    "\n",
    "def custom_alg_experiment(df_train, df_test, type, start_index=0, end_index=18): \n",
    "    X_train, y_train = get_X_and_Y(df_train, verbose=VERBOSE)\n",
    "    X_test, y_test = get_X_and_Y(df_test, verbose=VERBOSE)\n",
    "\n",
    "    k = 0\n",
    "\n",
    "    for hp_max_depth in hp_max_depth_values:\n",
    "        for hp_n_thresholds in hp_n_thresholds_values:\n",
    "            for hp_min_samples_split in hp_min_samples_split_values:\n",
    "\n",
    "                if k < start_index or k > end_index:\n",
    "                    k += 1\n",
    "                    continue\n",
    "\n",
    "                k += 1\n",
    "\n",
    "                printGreen(f\"Max depth: {hp_max_depth}\")\n",
    "                printGreen(f\"Number of thresholds for num attr: {hp_n_thresholds}\")\n",
    "                printGreen(f\"Min samples split: {hp_min_samples_split}\")\n",
    "\n",
    "                start_time = time.time()\n",
    "                decision_tree = CustomDecisionTree(max_depth=hp_max_depth, \n",
    "                                                    min_samples_split=hp_min_samples_split,\n",
    "                                                    num_thresholds_numerical_attr=hp_n_thresholds)\n",
    "                decision_tree.fit(X_train, y_train)\n",
    "                end_time = time.time()\n",
    "                decision_tree.create_dot_files(filename=f\"tree-custom-{type}-{hp_n_thresholds}-{hp_min_samples_split}\", generate_png=True, view=VIEW)\n",
    "                print(\"PERFORMANCES:\")\n",
    "                predictions = list(decision_tree.predict_test(X_test))\n",
    "            \n",
    "                calculate_performances(predictions, y_test, \"custom\", verbose=True)\n",
    "\n",
    "                print(\"\\nFit time: %.2f minutes\" % ((end_time - start_time) / 60))\n",
    "                print(\"Predict time: %.2f minutes\" % ((time.time() - end_time) / 60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPreprocessing: Undersampling\")\n",
    "df_train, df_test = original_df_train, original_df_test\n",
    "df_train = undersampling(df_train, VERBOSE=False)\n",
    "custom_alg_experiment(df_train, df_test, \"undersampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPreprocessing: Oversampling\")\n",
    "df_train, df_test = original_df_train, original_df_test\n",
    "df_train = oversampling(df_train, VERBOSE=False)\n",
    "custom_alg_experiment(df_train, df_test, \"oversampling\", start_index=0, end_index=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_alg_experiment(df_train, df_test, \"oversampling\", start_index=12, end_index=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C4.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def c45_experiment(df_train, df_test, type):\n",
    "    X_train, y_train = get_X_and_Y(df_train, verbose=VERBOSE)\n",
    "    X_test, y_test = get_X_and_Y(df_test, verbose=VERBOSE)\n",
    "\n",
    "    max_depths = [6, 9, 12]\n",
    "\n",
    "    for max_depth in max_depths:\n",
    "        printGreen(f\"Max depth: {max_depth}\")\n",
    "        start_time = time.time()\n",
    "        decision_tree = DecisionTreeC45(max_depth=max_depth)\n",
    "        decision_tree.fit(X_train, y_train)\n",
    "        end_time = time.time()\n",
    "        decision_tree.create_dot_files(filename=f\"tree-c45-{type}\", generate_png=True, view=VIEW)\n",
    "        print(\"PERFORMANCES:\")\n",
    "        predictions = list(decision_tree.predict_test(X_test))\n",
    "            \n",
    "        calculate_performances(predictions, y_test, \"c45\", verbose=True)\n",
    "\n",
    "        print(\"\\nFit time: %.2f minutes\" % ((end_time - start_time) / 60))\n",
    "        print(\"Predict time: %.2f minutes\" % ((time.time() - end_time) / 60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printLBlue(\"Preprocessing: Undersampling\")\n",
    "df_train, df_test = original_df_train, original_df_test\n",
    "df_train = undersampling(df_train, VERBOSE=False)\n",
    "c45_experiment(df_train, df_test, \"undersampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printLBlue(\"Preprocessing: Oversampling\")\n",
    "df_train, df_test = original_df_train, original_df_test\n",
    "df_train = oversampling(df_train, VERBOSE=False)\n",
    "c45_experiment(df_train, df_test, \"oversampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
